{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三次作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业我们加强对图神经网络的实践，包括GAT和GraphSAGE。具体地，我们需要（1）实现DGL中的GATConv，（2）利用DGL中的采样（Sampling）功能来部分完成GraphSAGE。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAT 代码填空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT里的聚合过程可以表示为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n",
    "\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中节点i和节点j之间的注意力分数计算如下。注意分母里的节点是包含节点$i$自身的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{i,j} =\n",
    "\\frac{\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
    "\\right)\\right)}\n",
    "{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
    "\\right)\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码并不遵循DGL的GATConv实现，而是按照上面公式实现的版本。DGL版本的GATConv使用了更加高效和更节省内存的实现，同时也更复杂和不容易理解，具体请参考链接：https://docs.dgl.ai/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv\n",
    "\n",
    "*注意：*在DGL里对于边上的softmax计算有专门的函数[dgl.nn.functional.edge_softmax](https://docs.dgl.ai/generated/dgl.nn.functional.edge_softmax.html#dgl.nn.functional.edge_softmax)。在完成作业的时候，需要使用`edge_softmax`实现对每个节点按边进行softmax计算$\\alpha_{i,j}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl import function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "\n",
    "\n",
    "class GATConv(nn.Module):\n",
    "    \"\"\"\n",
    "    参数说明\n",
    "    ------   \n",
    "    in_feats: 输入神经元的数量\n",
    "    out_feats: 输出神经元的数量\n",
    "    num_heads: 注意力机制head的数量\n",
    "    attn_drop: 对注意力分数的dropout概率\n",
    "    negative_slope: LeakyReLU中(-∞,0)的部分的斜率\n",
    "    activation: 激活函数\n",
    "    bias: 偏置项\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 attn_drop=0.,\n",
    "                 negative_slope=0.2,\n",
    "                 activation=None,\n",
    "                 bias=True):\n",
    "        super(GATConv, self).__init__()\n",
    "        self.in_feats=in_feats\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        self.fc = nn.Linear(self.in_feats, out_feats * num_heads, bias=False)\n",
    "\n",
    "        # 注意力机制中的参数\n",
    "        self.attn = nn.Parameter(torch.FloatTensor(size=(1, num_heads, 2 * out_feats)))  # concatinate后维度会变为2倍\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn, gain=gain)\n",
    "        if self.bias is not None:\n",
    "            nn.init.constant_(self.bias, 0)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "\n",
    "        # 对原始输入特征进行变换处理\n",
    "        feat_head = self.fc(feat).view(*feat.shape[:-1], self.num_heads, self.out_feats)\n",
    "        \n",
    "        #################\n",
    "\n",
    "        # 1. 把转换后的特征赋到点上。通过自定义lambda方法把源和邻居节点的特征concat起来，存到边上\n",
    "        #### 代码填空 ####\n",
    "                \n",
    "        # 2. 取出边上的特征，进行多头attention转换，并做LeakyRelu计算\n",
    "        #### 代码填空 ####\n",
    "\n",
    "        # 3. 使用DGL的edg_softmax函数完成按边的softmax计算\n",
    "        #### 代码填空 ####\n",
    "        \n",
    "        # 4. 对softmax值做dropout后，赋给边\n",
    "        #### 代码填空 ####\n",
    "        \n",
    "        # 5. 用softmax值和feat_head的特征，通过消息函数和聚合函数完成GAT的核心计算\n",
    "        #### 代码填空 ####\n",
    "        \n",
    "        # 6. 从节点特征里获取GAT的计算结果\n",
    "        #### 代码填空 ####\n",
    "        \n",
    "        ################\n",
    "            \n",
    "        # 使用bias项\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(*((1,) * len(feat.shape[:-1])), self.num_heads, self.out_feats)\n",
    "        \n",
    "        # 使用activation函数\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\" 2层GAT.\n",
    "    \n",
    "    参数说明\n",
    "    ----------\n",
    "    nfeat : 输入特征的维度\n",
    "    nhid : 隐藏神经元的数量\n",
    "    nclass : 输出神经元的数量，也即类别的数量\n",
    "    heads: 注意力机制中的head数量\n",
    "    attn_drop: 对注意力分数的dropout概率\n",
    "    activation: 使用的激活函数\n",
    "    with_bias: 是否带有偏置项\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 nfeat, \n",
    "                 nhid, \n",
    "                 nclass, \n",
    "                 heads=8, \n",
    "                 attn_drop=0.5, \n",
    "                 activation=F.elu, # 按照原论文的设置，我们使用ELu作为激活函数\n",
    "                 with_bias=True):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            in_feats=nfeat,\n",
    "            out_feats=nhid,\n",
    "            num_heads=heads,\n",
    "            attn_drop=attn_drop, \n",
    "            activation=activation,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            in_feats=nhid * heads,\n",
    "            out_feats=nclass,\n",
    "            num_heads=nclass,\n",
    "            attn_drop=attn_drop, \n",
    "            activation=activation,\n",
    "            bias=with_bias)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features).flatten(1)   # flatten的目的是把多头attention的输出变换成一个头\n",
    "        h = self.conv2(g, h).flatten(1)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"初始化GAT的参数.\n",
    "        \"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, lr=0.01, weight_decay=5e-4, epochs=200):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    #################\n",
    "    #### 代码填空 ####\n",
    "    ################\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, g):\n",
    "    \"\"\"测试模型在测试集上的性能\"\"\"\n",
    "    #################\n",
    "    #### 代码填空 ####\n",
    "    ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "Epoch 0, training loss: 3.891892671585083\n",
      "Epoch 10, training loss: 2.5167133808135986\n",
      "Epoch 20, training loss: 1.8516827821731567\n",
      "Epoch 30, training loss: 1.4740606546401978\n",
      "Epoch 40, training loss: 1.3435750007629395\n",
      "Epoch 50, training loss: 1.0301473140716553\n",
      "Epoch 60, training loss: 0.9806718826293945\n",
      "Epoch 70, training loss: 0.7711170315742493\n",
      "Epoch 80, training loss: 0.6751642227172852\n",
      "Epoch 90, training loss: 0.7746800184249878\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "dataset = CoraGraphDataset('./data') # 将数据保存在data文件夹下\n",
    "\n",
    "g = dataset[0]\n",
    "nclass = g.ndata['label'].max().item() + 1\n",
    "\n",
    "# 为了配合GAT的算法，给图上的节点添加自环的边\n",
    "g = dgl.remove_self_loop(g)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "gat = GAT(nfeat=g.ndata['feat'].shape[1],\n",
    "      nhid=8, heads=8, nclass=nclass)\n",
    "train(gat, g, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.8297 accuracy= 0.8170\n"
     ]
    }
   ],
   "source": [
    "preds, output, acc = test(gat, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GraphSage 代码填空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE的核心部分是采样（sampling）。我们利用DGL提供的MultiLayerNeighborSampler来实现采样功能，并使用DGL提供的NodeDataLoader来完成迷你批次样本的构建。\n",
    "\n",
    "*注意：*MultiLayerNeighborSampler的入参fanouts表示的是对于每一阶邻居的采样的数量。例如，fanouts=\\[10,5\\]表示从一阶邻居里随机地采样10个邻居，再从这10个邻居的邻居(二阶邻居)里，随机采样5个邻居。更多的含义可以查看[DGL的MultiLayerNeighborSampler文档](https://docs.dgl.ai/api/python/dgl.dataloading.html#neighbor-sampler)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的代码填空很少，主要是希望同学们能够通过例子来学会使用MultiLayerNeighborSampler。关于使用DGL进行大图迷你批次训练的内容，可以查看DGL[《用户指南》](https://docs.dgl.ai/guide_cn/index.html)里的[第6章：在大图上的随机（批次）训练](https://docs.dgl.ai/guide_cn/minibatch.html)的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外需要注意的是GraphSAGE中的聚合方式，它有两个变换矩阵：\n",
    "\n",
    "$$ \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.dataloading import MultiLayerNeighborSampler\n",
    "from dgl.dataloading import NodeDataLoader\n",
    "\n",
    "dataset = CoraGraphDataset('./data') # 将数据保存在data文件夹下\n",
    "\n",
    "g = dataset[0]\n",
    "\n",
    "train_idx = torch.arange(g.num_nodes())[g.ndata['train_mask']]\n",
    "\n",
    "sampler = MultiLayerNeighborSampler(fanouts=[10,5])\n",
    "train_loader = NodeDataLoader(g, train_idx,\n",
    "                              sampler, batch_size=128,\n",
    "                              shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\" 2层GraphSAGE\n",
    "    \n",
    "    参数说明\n",
    "    ----------\n",
    "    nfeat : 输入特征的维度\n",
    "    nhid : 隐藏神经元的数量\n",
    "    nclass : 输出神经元的数量，也即类别的数量\n",
    "    dropout : dropout中的概率\n",
    "    with_bias: 是否带有偏置项\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        # 这里我们使用平均`mean`来聚合邻居的特征，DGL的SAGEConv还有其他的聚合方法选择，比如`pool`,`lstm`。\n",
    "        self.convs.append(SAGEConv(nfeat, nhid, aggregator_type='mean', bias=with_bias, activation=F.relu))\n",
    "        self.convs.append(SAGEConv(nhid, nclass, aggregator_type='mean', bias=with_bias))\n",
    "        \n",
    "        self.droput = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"初始化模型参数\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, blocks, features):\n",
    "        \n",
    "        h = self.convs[0](blocks[0], features)\n",
    "        h = F.dropout(h, p=self.droput, training=self.training)\n",
    "        h = self.convs[1](blocks[1], h)\n",
    "\n",
    "        return h.log_softmax(dim=-1)       \n",
    "            \n",
    "    def inference(self, g, features):\n",
    "        \"\"\"模型测试阶段的前向传播，不采样邻居节点，直接使用所有的邻居。可参考GCN中forward()的实现\"\"\"\n",
    "        #################\n",
    "        #### 代码填空 ####\n",
    "        ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, train_loader, epochs, device='cpu', lr=0.01, weight_decay=5e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    features = g.ndata['feat'].to(device)\n",
    "    labels = g.ndata['label'].to(device)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        for input_nodes, seeds, mfgs in train_loader:\n",
    "            # 'input_nodes' 是采样子图里所有节点的ID\n",
    "            # 'seeds'是采样的种子节点ID，也是需要预测和计算loss的节点\n",
    "            # 'mfgs':采样后的多层子图\n",
    "            batch_inputs = features[input_nodes]  # 获取子图所有节点的特征\n",
    "            batch_labels = labels[seeds]            # 获取种子节点对应的标签\n",
    "            \n",
    "            mfgs = [mfg.to(device) for mfg in mfgs]\n",
    "\n",
    "            out = model(mfgs, batch_inputs)\n",
    "            loss = F.nll_loss(out, batch_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        if it % 10 ==0:\n",
    "            print('Epoch:', it, 'training loss:', total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model):\n",
    "    \"\"\"Evaluate GNN performance on test set.\n",
    "    \"\"\"\n",
    "    model.eval() # eval()把dropout的概率设置为0（不使用dropout）\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    labels = g.ndata['label'] \n",
    "    features = g.ndata['feat']\n",
    "    output = model.inference(g, features) # 得到模型输出\n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) # 得到预测值\n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() # 得到准确率\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Epoch: 0 training loss: 3.8636229038238525\n",
      "Epoch: 10 training loss: 2.7999221086502075\n",
      "Epoch: 20 training loss: 1.5960562229156494\n",
      "Epoch: 30 training loss: 0.9994849264621735\n",
      "Epoch: 40 training loss: 0.8969895541667938\n",
      "Epoch: 50 training loss: 0.6504308879375458\n",
      "Epoch: 60 training loss: 0.5231107324361801\n",
      "Epoch: 70 training loss: 0.3678404539823532\n",
      "Epoch: 80 training loss: 0.36875565350055695\n",
      "Epoch: 90 training loss: 0.3202539086341858\n"
     ]
    }
   ],
   "source": [
    "print(nclass)\n",
    "\n",
    "sage = GraphSAGE(nfeat=g.ndata['feat'].shape[1], nhid=16, nclass=nclass)\n",
    "train(sage, g, train_loader, epochs=100, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7128 accuracy= 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, output, acc_test = test(sage)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dgl]",
   "language": "python",
   "name": "conda-env-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
