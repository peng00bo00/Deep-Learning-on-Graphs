{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"hw3.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaT99hONxNvO","executionInfo":{"status":"ok","timestamp":1636090132061,"user_tz":-240,"elapsed":9613,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}},"outputId":"a2e847c0-e3c6-428f-f825-bf9df48c5b95"},"source":["## install torch-geometric in colab\n","!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","!pip install torch-geometric"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n","Looking in links: https://data.pyg.org/whl/torch-1.9.0+cu111.html\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"vrGLI1n7xEGh"},"source":["# 第三次作业"]},{"cell_type":"markdown","metadata":{"id":"IH9-5bQWxEGk"},"source":["本次作业我们加强对图神经网络的实践，包括GAT和GraphSAGE。具体地，我们需要（1）实现PyG中的GATConv，（2）利用PyG中的采样（Sampling）功能来完成GraphSAGE。"]},{"cell_type":"markdown","metadata":{"id":"RKVei8DmxEGk"},"source":["## 1. GAT 代码填空"]},{"cell_type":"markdown","metadata":{"id":"NmXymfXWxEGl"},"source":["GAT里的聚合过程可以表示为"]},{"cell_type":"markdown","metadata":{"id":"USL33Hs8xEGl"},"source":["$$\\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n","\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j}$$"]},{"cell_type":"markdown","metadata":{"id":"sW4MYfqXxEGl"},"source":["其中节点i和节点j之间的注意力分数为"]},{"cell_type":"markdown","metadata":{"id":"OPKb33uXxEGm"},"source":["$$\\alpha_{i,j} =\n","\\frac{\n","\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n","\\right)\\right)}\n","{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n","\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n","\\right)\\right)}.$$"]},{"cell_type":"markdown","metadata":{"id":"7CGZD0v1xEGm"},"source":["下面的代码改编自PyG某一版本的GATConv实现。我做了诸多简化来让它简单易读且适应于当前版本。\n","\n","注：这里我们没有用最新版PyG中GATConv的实现，因为最新版本的GATConv不是特别好懂。\n","\n","实际上我们参考的是1.3.2版本的GATConv，见该链接：https://github.com/pyg-team/pytorch_geometric/blob/881d5ba2aefc26328eeeaa17fd7ef6daaae06ef4/torch_geometric/nn/conv/gat_conv.py"]},{"cell_type":"code","metadata":{"id":"2VTRzNurxEGn","executionInfo":{"status":"ok","timestamp":1636090133392,"user_tz":-240,"elapsed":1339,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["import torch\n","from torch import Tensor\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","from torch_sparse import SparseTensor, set_diag\n","from torch_geometric.nn.dense.linear import Linear\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.nn.inits import glorot, zeros\n","import torch.optim as optim\n","\n","class GATConv(MessagePassing):\n","    \"\"\"\n","    参数说明\n","    ------   \n","    in_channels: 输入神经元的数量\n","    out_channels: 输出神经元的数量\n","    heads: 注意力机制head的数量\n","    concat: 如果concat是True，那么最后的输出就是拼接每个head的输出；如果concat是False，\n","        那么最后的输出就是对每个head的输出求平均.\n","    negative_slope: LeakyReLU中(-∞,0)的部分的斜率。\n","    dropout: 对注意力分数的dropout概率。\n","    bias: 偏置项\n","    \"\"\"\n","\n","    def __init__(self, in_channels,\n","                 out_channels, heads=1, concat=True,\n","                 negative_slope=0.2, dropout=0.0,\n","                 bias=True, **kwargs):\n","        \n","        kwargs.setdefault('aggr', 'add')\n","        super(GATConv, self).__init__(node_dim=0, **kwargs)\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.heads = heads\n","        self.concat = concat\n","        self.negative_slope = negative_slope\n","        self.dropout = dropout\n","\n","        self.lin = Linear(in_channels, heads * out_channels,\n","                              bias=False, weight_initializer='glorot')\n","\n","        # 注意力机制中的参数\n","        self.att = Parameter(torch.Tensor(1, heads, 2*out_channels))\n","\n","        if bias and concat:\n","            self.bias = Parameter(torch.Tensor(heads * out_channels))\n","        elif bias and not concat:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        \"\"\"初始化参数\"\"\"\n","        self.lin.reset_parameters()\n","        glorot(self.att)\n","        zeros(self.bias)\n","    \n","    def forward(self, x, edge_index, size=None):\n","        \"\"\"前向传播\"\"\"\n","        edge_index, _ = remove_self_loops(edge_index)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n","        \n","        x = self.lin(x)\n","        ## x[N, self.heads * self.out_channels]\n","        output = self.propagate(edge_index, size=size, x=x) # 得到聚合信息后的节点特征\n","        ## output[N, self.heads, self.out_channels]\n","        \n","        if self.concat is True:\n","            #################\n","            #### 代码填空 ####\n","            ################\n","            output = output.view(-1, self.heads * self.out_channels)\n","        else:\n","            #################\n","            #### 代码填空 ####\n","            ################\n","            output = output.mean(dim=1)\n","\n","        if self.bias is not None:\n","            output = output + self.bias\n","        return output\n","\n","    def message(self, edge_index_i, x_i, x_j, size_i):\n","        \"\"\"计算注意力分数。\n","    \n","        参数说明\n","        ----\n","        edge_index_i: 边的序号的第一维，对应x_i的邻居节点\n","        x_i: source节点的节点特征\n","        x_j: target节点的节点特征\n","        size_i: source节点的节点数量\n","        \"\"\"\n","        \n","        ###############################################\n","        #### 代码填空，计算softmax之前的注意力分数alpha ####\n","        ###############################################\n","\n","        x_j = x_j.view(-1, self.heads, self.out_channels)\n","        x_i = x_i.view(-1, self.heads, self.out_channels)\n","\n","        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n","\n","        alpha = F.leaky_relu(alpha, self.negative_slope)\n","        alpha = softmax(src=alpha, index=edge_index_i, num_nodes=size_i)\n","        \n","        # 对注意力分数alpha进行dropout\n","        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n","        \n","        ##################################\n","        #### 代码填空，完成需要返回的变量 ####\n","        #################################\n","\n","        ## from node j to node i\n","        return x_j * alpha.view(-1, self.heads, 1)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OdemifNxEGp","executionInfo":{"status":"ok","timestamp":1636090133392,"user_tz":-240,"elapsed":3,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["class GAT(torch.nn.Module):\n","    \"\"\" 2层GAT.\n","    \n","    参数说明\n","    ----------\n","    nfeat : 输入特征的维度\n","    nhid : 隐藏神经元的数量\n","    nclass : 输出神经元的数量，也即类别的数量\n","    heads: 注意力机制中的head数量\n","    output_heads: 输出层的head数量\n","    dropout : dropout中的概率\n","    with_bias: 是否带有偏置项\n","    \"\"\"\n","\n","    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0.5, with_bias=True):\n","\n","        super(GAT, self).__init__()\n","\n","        self.conv1 = GATConv(\n","            nfeat,\n","            nhid,\n","            heads=heads,\n","            dropout=dropout,\n","            bias=with_bias)\n","\n","        self.conv2 = GATConv(\n","            nhid * heads,\n","            nclass,\n","            heads=output_heads,\n","            concat=False,\n","            dropout=dropout,\n","            bias=with_bias)\n","\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = F.elu(self.conv1(x, edge_index)) # 按照原论文的设置，我们使用ELu作为激活函数\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return F.log_softmax(x, dim=1)\n","\n","    def initialize(self):\n","        \"\"\"初始化GAT的参数.\n","        \"\"\"\n","        self.conv1.reset_parameters()\n","        self.conv2.reset_parameters()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3CuPwFMxEGp","executionInfo":{"status":"ok","timestamp":1636090133393,"user_tz":-240,"elapsed":3,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200):\n","    \"\"\"训练模型\"\"\"\n","    #################\n","    #### 代码填空 ####\n","    ################\n","\n","    ## copy from sample codes\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    labels = data.y\n","    train_mask = data.train_mask\n","\n","    for i in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        output = model(data)\n","\n","        loss = F.nll_loss(output[train_mask], labels[train_mask]) # 用训练集中的节点来计算损失函数\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 10 == 0:\n","            print('Epoch {}, training loss: {}'.format(i, loss.item()))\n","\n","@torch.no_grad()\n","def test(model, data):\n","    \"\"\"测试模型在测试集上的性能\"\"\"\n","    #################\n","    #### 代码填空 ####\n","    ################\n","\n","    ## copy from sample codes\n","    model.eval()\n","    test_mask = data.test_mask\n","    labels = data.y \n","    output = model(data) # 得到模型输出\n","    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n","    preds = output[test_mask].argmax(1) # 得到预测值\n","    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() # 得到准确率\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test))\n","    return preds, output, acc_test.item()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VM6aLJ0xEGq","executionInfo":{"status":"ok","timestamp":1636090141014,"user_tz":-240,"elapsed":7624,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}},"outputId":"d7e3f21e-f16c-46a4-8f13-2ccaffcaaa83"},"source":["from torch_geometric.datasets import Planetoid\n","dataset = Planetoid(root='./data', name='Cora') # 将数据保存在data文件夹下\n","data = dataset[0]\n","nclass = data.y.max().item()+1\n","gat = GAT(nfeat=data.x.shape[1],\n","      nhid=8, heads=8, nclass=nclass)\n","train(gat, data, epochs=100)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, training loss: 1.9829869270324707\n","Epoch 10, training loss: 0.3864467442035675\n","Epoch 20, training loss: 0.39203497767448425\n","Epoch 30, training loss: 0.21190983057022095\n","Epoch 40, training loss: 0.24609829485416412\n","Epoch 50, training loss: 0.21547135710716248\n","Epoch 60, training loss: 0.1505132019519806\n","Epoch 70, training loss: 0.18310952186584473\n","Epoch 80, training loss: 0.18345752358436584\n","Epoch 90, training loss: 0.17470145225524902\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSAqSb5_xEGq","executionInfo":{"status":"ok","timestamp":1636090141015,"user_tz":-240,"elapsed":10,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}},"outputId":"4872c595-46bd-4b7b-f494-11a4fb48efd0"},"source":["preds, output, acc = test(gat, data)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Test set results: loss= 0.7024 accuracy= 0.7970\n"]}]},{"cell_type":"markdown","metadata":{"id":"Vc0vYxsRxEGr"},"source":["## 2. GraphSAGE 代码填空"]},{"cell_type":"markdown","metadata":{"id":"i72Evq7uxEGr"},"source":["GraphSAGE的核心部分是采样（sampling）。我们利用PyG提供的NeighborSampler来实现采样功能。"]},{"cell_type":"markdown","metadata":{"id":"zIl4WFD2xEGr"},"source":["这部分的代码填空很少，主要是希望同学们能够通过例子来学会使用NeighborSampler。"]},{"cell_type":"markdown","metadata":{"id":"9hqoP-OexEGr"},"source":["另外需要注意的是GraphSAGE中的聚合方式，它有两个变换矩阵：\n","\n","$$ \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n","        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n","$$"]},{"cell_type":"code","metadata":{"id":"hygLIQvjxEGr","executionInfo":{"status":"ok","timestamp":1636090141015,"user_tz":-240,"elapsed":9,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["from torch_geometric.datasets import Planetoid\n","from torch_geometric.loader import NeighborSampler\n","dataset = Planetoid(root='./data', name='Cora') # 将数据保存在data文件夹下\n","data = dataset[0]\n","nclass = data.y.max().item()+1\n","\n","sizes=[10,5] # 表示第一层采样10个邻居，第二层采样5个邻居\n","train_idx = torch.arange(data.num_nodes)[data.train_mask]\n","train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n","                               sizes=sizes, batch_size=128,\n","                               shuffle=True, num_workers=0) "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JZu578JxEGs","executionInfo":{"status":"ok","timestamp":1636090141015,"user_tz":-240,"elapsed":8,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["from torch_geometric.nn import SAGEConv\n","\n","class GraphSAGE(torch.nn.Module):\n","    \"\"\" 2层GraphSAGE\n","    \n","    参数说明\n","    ----------\n","    nfeat : 输入特征的维度\n","    nhid : 隐藏神经元的数量\n","    nclass : 输出神经元的数量，也即类别的数量\n","    dropout : dropout中的概率\n","    with_bias: 是否带有偏置项\n","    \"\"\"\n","\n","    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n","        super(GraphSAGE, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.append(SAGEConv(nfeat, nhid, bias=with_bias))\n","        self.convs.append(SAGEConv(nhid, nclass, bias=with_bias))\n","        self.dropout = dropout\n","        \n","    def reset_parameters(self):\n","        \"\"\"初始化模型参数\"\"\"\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","            conv.reset_parameters()\n","\n","    def forward(self, x, adjs):\n","        \"\"\"对应于neighborsampler的前向传播\"\"\"\n","        num_layers = len(adjs)\n","        for i, (edge_index, _, size) in enumerate(adjs):\n","            x_target = x[:size[1]]  # x_target是目标节点，最后一层的目标节点就是带标签的节点\n","            x = self.convs[i]((x, x_target), \n","                              edge_index) # x是邻居节点的特征，x_target是目标节点的特征，它们对应不同的特征变换矩阵\n","            if i != num_layers - 1:\n","                x = F.relu(x)\n","                x = F.dropout(x, p=self.dropout, training=self.training)\n","        return x.log_softmax(dim=-1)       \n","            \n","    def inference(self, data):\n","        \"\"\"模型测试阶段的前向传播，不采样邻居节点，直接使用所有的邻居。可参考GCN中forward()的实现\"\"\"\n","        #################\n","        #### 代码填空 ####\n","        ################\n","\n","        x, edge_index = data.x, data.edge_index\n","\n","        x = F.relu(self.convs[0](x, edge_index))\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = F.relu(self.convs[1](x, edge_index))\n","\n","        return x.log_softmax(dim=-1) "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5J4SB9JxEGs","executionInfo":{"status":"ok","timestamp":1636090141814,"user_tz":-240,"elapsed":807,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["def train(model, train_loader, epochs, device='cpu', lr=0.01, weight_decay=5e-4):\n","    \"\"\"训练阶段，这部分我们就不再设置填空作业，而是选择让同学们自行理解\"\"\"\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    \n","    x = data.x.to(device)\n","    y = data.y.squeeze().to(device)\n","    \n","    for it in range(epochs):\n","        model.train()\n","\n","        total_loss = 0\n","        for batch_size, n_id, adjs in train_loader:\n","            # `n_id`是被采样的节点（包含了有标签的节点，和无标签的邻居节点）\n","            # `adjs`里面对应了每一层采样的邻接矩阵，包含了`(edge_index, e_id, size)`\n","            adjs = [adj.to(device) for adj in adjs]\n","\n","            optimizer.zero_grad()\n","            out = model(x[n_id], adjs)\n","            loss = F.nll_loss(out, y[n_id[:batch_size]]) # n_id[:batch_size]表示的采样的带标签的节点\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        loss = total_loss / len(train_loader)\n","        if it % 10 ==0:\n","            print('Epoch:', it, 'training loss:', total_loss)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHx3WtiYxEGs","executionInfo":{"status":"ok","timestamp":1636090141814,"user_tz":-240,"elapsed":4,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}}},"source":["@torch.no_grad()\n","def test(model):\n","    \"\"\"测试模型在测试集上的性能\"\"\"\n","    model.eval() # eval()把dropout的概率设置为0（不使用dropout）\n","    test_mask = data.test_mask\n","    labels = data.y \n","    output = model.inference(data) # 得到模型输出\n","    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n","    preds = output[test_mask].argmax(1) # 得到预测值\n","    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() # 得到准确率\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test))\n","    return preds, output, acc_test.item()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAKPs8cHxEGs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636090145234,"user_tz":-240,"elapsed":3424,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}},"outputId":"8fa045c1-ab8d-4405-adbf-5c146065c9f4"},"source":["sage = GraphSAGE(nfeat=data.x.shape[1], nhid=16, nclass=nclass)\n","train(sage, train_loader, epochs=100, device='cpu')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 training loss: 3.8576111793518066\n","Epoch: 10 training loss: 0.44551892578601837\n","Epoch: 20 training loss: 0.167935810983181\n","Epoch: 30 training loss: 0.09276534151285887\n","Epoch: 40 training loss: 0.06609238497912884\n","Epoch: 50 training loss: 0.02603654097765684\n","Epoch: 60 training loss: 0.04685967415571213\n","Epoch: 70 training loss: 0.04303382057696581\n","Epoch: 80 training loss: 0.09111708216369152\n","Epoch: 90 training loss: 0.11578271351754665\n"]}]},{"cell_type":"code","metadata":{"id":"1iwrv_s5xEGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636090145234,"user_tz":-240,"elapsed":4,"user":{"displayName":"Bo Peng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01149616516536783206"}},"outputId":"ddb712af-3e72-46a8-8d0e-e1eb6216acf1"},"source":["pred, output, acc_test = test(sage)\n","acc_test"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Test set results: loss= 0.9304 accuracy= 0.7510\n"]},{"output_type":"execute_result","data":{"text/plain":["0.751"]},"metadata":{},"execution_count":12}]}]}